{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b8eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e99f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "from itertools import combinations\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "file_paths = {\n",
    "    \"AMAZON\": r\"C:\\Users\\Hem\\Downloads\\AMAZON.csv\",\n",
    "    \"COSTCO\": r\"C:\\Users\\Hem\\Downloads\\COSTCO.csv\",\n",
    "    \"DMART\": r\"C:\\Users\\Hem\\Downloads\\DMART.csv\",\n",
    "    \"WALMART\": r\"C:\\Users\\Hem\\Downloads\\WALMART.csv\",\n",
    "    \"KMART\": r\"C:\\Users\\Hem\\Downloads\\KMART.csv\"\n",
    "}\n",
    "\n",
    "# Extract transactions from CSV files\n",
    "def load_transactions(file_path):\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        transactions = [list(filter(None, row)) for row in reader]  # Filter out empty items in rows\n",
    "    return transactions\n",
    "\n",
    "# Applyinng Brute Force method to generate frequent items\n",
    "def generate_frequent_itemsets(transactions, support_threshold):\n",
    "    item_count = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_count[item] = item_count.get(item, 0) + 1\n",
    "\n",
    "    frequent_itemsets = {1: {item: count for item, count in item_count.items() if count / len(transactions) >= support_threshold}}\n",
    "\n",
    "    k = 2\n",
    "    while True:\n",
    "        prev_itemsets = list(frequent_itemsets[k - 1].keys())\n",
    "        new_itemsets = list(combinations(prev_itemsets, k))\n",
    "        item_count = {}\n",
    "        for transaction in transactions:\n",
    "            transaction_set = set(transaction)\n",
    "            for itemset in new_itemsets:\n",
    "                if set(itemset).issubset(transaction_set):\n",
    "                    item_count[itemset] = item_count.get(itemset, 0) + 1\n",
    "\n",
    "        frequent_itemsets[k] = {itemset: count for itemset, count in item_count.items() if count / len(transactions) >= support_threshold}\n",
    "        if not frequent_itemsets[k]:\n",
    "            del frequent_itemsets[k]\n",
    "            break\n",
    "        k += 1\n",
    "    return frequent_itemsets\n",
    "\n",
    "# Applying Apriori Algorithm\n",
    "def apriori_algorithm(transactions, support_threshold, confidence_threshold):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df, min_support=support_threshold, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=confidence_threshold)\n",
    "\n",
    "    return frequent_itemsets, rules\n",
    "\n",
    "# Applying FP-Growth Algorithm\n",
    "def fpgrowth_algorithm(transactions, support_threshold, confidence_threshold):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = fpgrowth(df, min_support=support_threshold, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=confidence_threshold)\n",
    "\n",
    "    return frequent_itemsets, rules\n",
    "\n",
    "# Comparing by timing function\n",
    "def measure_execution_time(algorithm_func, *args):\n",
    "    start_time = time.time()\n",
    "    result = algorithm_func(*args)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "# Source code\n",
    "while True:\n",
    "    # user defined entry or exit\n",
    "    print(\"\\nAvailable databases:\")\n",
    "    for i, name in enumerate(file_paths.keys(), 1):\n",
    "        print(f\"{i}. {name}\")\n",
    "    print(\"0. Exit\")\n",
    "    \n",
    "    choice = int(input(\"Enter the number corresponding to the database you'd like to choose (or 0 to exit): \"))\n",
    "\n",
    "    # Exit the loop if the user chooses 0\n",
    "    if choice == 0:\n",
    "        print(\"Exiting the program.\")\n",
    "        break\n",
    "\n",
    "    # selected database \n",
    "    db_name = list(file_paths.keys())[choice - 1]\n",
    "\n",
    "    # Load the selected transactions\n",
    "    transactions = load_transactions(file_paths[db_name])\n",
    "    print(f\"Loaded {len(transactions)} transactions from {db_name}.\")\n",
    "\n",
    "    # user-defined for support and confidence thresholds\n",
    "    support_threshold = float(input(\"Enter support threshold in % (e.g., 10 for 10%): \")) / 100\n",
    "    confidence_threshold = float(input(\"Enter confidence threshold in % (e.g., 20 for 20%): \")) / 100\n",
    "\n",
    "    print(f\"\\nProcessing {db_name} with support {support_threshold * 100}% and confidence {confidence_threshold * 100}%...\")\n",
    "\n",
    "    # Brute Force\n",
    "    bf_result, bf_time = measure_execution_time(generate_frequent_itemsets, transactions, support_threshold)\n",
    "    print(f\"\\nBrute Force Frequent Itemsets:\\n{bf_result}\")\n",
    "    print(f\"Brute Force Time: {bf_time:.4f}s\")\n",
    "\n",
    "    # Apriori\n",
    "    apriori_result, apriori_time = measure_execution_time(apriori_algorithm, transactions, support_threshold, confidence_threshold)\n",
    "    print(f\"\\nApriori Frequent Itemsets:\\n{apriori_result[0]}\")\n",
    "    print(f\"Apriori Rules:\\n{apriori_result[1]}\")\n",
    "    print(f\"Apriori Time: {apriori_time:.4f}s\")\n",
    "\n",
    "    # FP-Growth\n",
    "    fp_result, fp_time = measure_execution_time(fpgrowth_algorithm, transactions, support_threshold, confidence_threshold)\n",
    "    print(f\"\\nFP-Growth Frequent Itemsets:\\n{fp_result[0]}\")\n",
    "    print(f\"FP-Growth Rules:\\n{fp_result[1]}\")\n",
    "    print(f\"FP-Growth Time: {fp_time:.4f}s\")\n",
    "\n",
    "    # If user wants to analyze different dataset\n",
    "    continue_choice = input(\"\\nDo you want to analyze another dataset? (yes/no): \").strip().lower()\n",
    "    if continue_choice != 'yes':\n",
    "        print(\"Exiting the program.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621ed0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9c85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6d30ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
